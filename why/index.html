<h1 id="why-oboe-js-">Why Oboe.js?</h1>
<p>This page was written to show how streaming can speed up applications. The examples 
illustrated show web interfaces using AJAX to pull in new data but the same techniques
would apply equally well anywhere that REST is used.</p>
<h2 id="stream-any-json-rest-resource">Stream any JSON REST resource</h2>
<p>Let&#39;s start by examining the standard pattern found on most AJAX-powered sites.
We have a client-side web application and a service that provides it with data.
The page isn&#39;t updated until the response completes:</p>
<p>{{demo &quot;fast-ajax-discrete&quot;}}</p>
<p>Oboe is different from most streaming JSON libraries since the JSON
does not have to follow a special format. 
On a good connection there isn&#39;t a lot of time to save but 
it is likely that <a href="http://www.sigchi.org/chi95/proceedings/shortppr/egd_bdy.htm">progressive display in itself will improve the <em>perception</em> of 
performance</a>:</p>
<p>{{demo &quot;fast-ajax-progressive&quot;}}</p>
<p>With a slower connection or larger data the improvement
is more significant.</p>
<h2 id="transmit-fluently-over-mobile">Transmit fluently over mobile</h2>
<p>Mobile networks today are high-bandwidth but can also be
high-latency and come with inconsistent packet delivery times.
This is why buffered content like streaming <abbr title="high definition">HD</abbr> video plays
fluidly but web surfing still feels laggy. The visualisation
below approximates a medium-sized download on a mobile network:</p>
<p>{{demo &quot;mobile-discrete&quot;}}</p>
<p>Oboe.js makes it easy for the programmer to use chunks from the response as soon 
as they arrive. This helps webapps to feel faster when running over mobile networks:</p>
<p>{{demo &quot;mobile-progressive&quot;}}</p>
<h2 id="handle-dropped-connections-with-grace">Handle dropped connections with grace</h2>
<p>Oboe.js provides improved tolerance if a connection is lost before
the response completes.
Most AJAX frameworks equate a dropped connection with total failure and discard
the partially transferred data, even if 90% was received correctly.</p>
<p>We can handle this situation better by using the partially transferred data
instead of throwing it away. Given an incremental approach to parsing, using partial data
follows naturally without requiring any extra programming. </p>
<p>In the next visualisation we have a mobile connection which fails when the
user enters a building:</p>
<p>{{demo &quot;mobile-fail-discrete&quot;}}</p>
<p>Because Oboe.js views the HTTP response as a
series of small, useful parts, when a connection is lost it is simply
the case that some parts were successful and were used already,
while others did not arrive. Fault tolerance comes for free.</p>
<p>In the example below the client is smart enough so that when the network
comes back it only requests the data that was missed on the first attempt:</p>
<p>{{demo &quot;mobile-fail-progressive&quot;}}</p>
<h2 id="streamline-resource-aggregation">Streamline resource aggregation</h2>
<p>It is a common pattern for web clients to retrieve data through a middle tier.
Nodes in the middle tier connect to multiple back-end services and
create a single, aggregated response by combining their data.</p>
<p>The visualisation below shows an example without streaming.
<span class="server2">Origin 1</span> is slower
than
<span class="server1">Origin 2</span>
but the 
<span class="aggregator">aggregator</span> is forced to respond at the speed of
<span class="server2">the slowest service</span>:</p>
<p>{{demo &quot;aggregated-discrete&quot;}}</p>
<p>We can speed this scenario up by using Oboe.js to load data in
<span class="aggregator">the aggregator</span> and 
<span class="place">the client</span>.
The aggregator dispatches the data as soon as it has it and 
the client displays the data as soon as it is arrives.</p>
<p>{{demo &quot;aggregated-progressive&quot;}}</p>
<p>Despite being a stream, 
<span class="aggregator">the aggregator&#39;s</span>
output is 100% valid JSON so it remains compatible 
with standard AJAX tools. A client using a streaming parser like Oboe.js
consumes the resource as a stream but a more traditional client has no 
problem reading it as a static resource.</p>
<p>In a Java stack this could also be implemented by using 
<a href="http://code.google.com/p/google-gson/">GSON</a> in the middle tier.</p>
<h2 id="step-outside-the-trade-off-between-big-and-small-json">Step outside the trade-off between big and small JSON</h2>
<p>There is often a tradeoff using traditional REST clients:</p>
<ul>
<li>Request too much data and the application feels unresponsive because each request
takes some time to download.</li>
<li>Request less and, while the first data is handled earlier, more requests are needed,
meaning a greater http overhead and more time overall.</li>
</ul>
<p>Oboe.js breaks out of the tradeoff by beating both.
Large resources load just as responsively as smaller ones so the developer can request more
and let it stream. </p>
<p>In the visualisation below three rival clients
connect to <span class="place">the same server</span>. The
<span class="client1">top client requests a lot of data</span>,
<span class="client2">the middle a little data twice</span>, and
<span class="client3">the bottom a lot using Oboe.js</span>:</p>
<p>{{demo &quot;big-small&quot;}}</p>
<h2 id="send-historic-and-live-data-using-the-same-transport">Send historic and live data using the same transport</h2>
<p>It is a common pattern in web interfaces to fetch existing data
and then keep the page updated with &#39;live&#39; events as they happen.
We traditionally use two transports here but
wouldn&#39;t our day be easier if we didn&#39;t have to program distinct cases?</p>
<p>In the example below the message server intentionally writes a JSON response
that never completes. It starts by writing out the existing messages
as a chunk and then continues to write out new ones as they happen.
The only difference between &#39;old&#39; and &#39;new&#39; data is timing:</p>
<p>{{demo &quot;historic-and-live&quot;}}</p>
<h2 id="publish-cacheable-streamed-content">Publish cacheable, streamed content</h2>
<p><a href="#send-historic-and-live-data-using-the-same-transport">Above</a> we had a
service where the response intentionally never completes. Here we will
consider a slightly different case: JSON that streams to reflect
live events but which eventually ends.</p>
<p>Most streaming HTTP techniques like Websockets intentionally avoid caches
and proxies.
Oboe.js is different; by taking a REST-based approach to streaming it remains
compatible with HTTP intermediaries and can take advantage of caches to better
distribute the content.</p>
<p>The visualisation below is based on <a href="http://en.wikipedia.org/wiki/File:Cartogram%E2%80%942012_Electoral_Vote.svg">a cartogram taken from
Wikipedia</a>
and simulates each state&#39;s results being announced in the <a href="http://en.wikipedia.org/wiki/United_States_presidential_election,_2012">2012 United
States presidential
election</a>.
Time is sped up so that hours are condensed into seconds.</p>
<p>{{demo &quot;caching&quot;}}</p>
<p>This won&#39;t work for every use case. Websockets remains the better choice where
live data after-the-fact is no longer interesting. REST-based Cacheable streaming
works best for cases where the live data is not specific to a single user and remains
interesting as it ages.</p>
<h2 id="what-downsides-">What downsides?</h2>
<p>Because it is a pure Javascript parser, Oboe.js requires more CPU time
than JSON.parse. Oboe.js works marginally more
slowly for small messages that load very quickly 
but for most real-world cases using i/o effectively beats optimising CPU time.</p>
<p>SAX parsers require less memory than Oboe&#39;s pattern-based parsing model because
they do not build up a parse tree. See <a href="parsers">Oboe.js vs SAX vs DOM</a>. </p>
<p>If in doubt, benchmark, but don&#39;t forget to
use the real internet, including mobile, and think about perceptual performance.</p>
